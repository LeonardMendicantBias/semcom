from typing import Optional, Tuple
from functools import partial

import torch
from torch import nn
import torch.nn.functional as F


class VectorQuantizer(nn.Module):
    
    def __init__(
        self,
        embed_dim: int,
        n_embed: int,
        beta: float=0.25,
        use_norm: bool=True,
        use_residual: bool=False,
        num_quantizers: Optional[int] = None,
    ) -> None:
        super().__init__()
        self.straight_through = True

        self.norm = lambda x: F.normalize(x, dim=-1) if use_norm else x

        self.use_residual = use_residual
        self.num_quantizers = num_quantizers

        self.embed_dim = embed_dim
        self.n_embed = n_embed

        self.embedding = nn.Embedding(self.n_embed, self.embed_dim)
        self.embedding.weight.data.normal_()

        self.beta = beta

    def quantize(self,
        z: torch.FloatTensor
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor]:
        z_reshaped_norm = self.norm(z.view(-1, self.embed_dim))
        embedding_norm = self.norm(self.embedding.weight)

        d = (
            torch.sum(z_reshaped_norm**2, dim=1, keepdim=True) +
            torch.sum(embedding_norm**2, dim=1) -
            2*torch.einsum("b d, n d -> b n", z_reshaped_norm, embedding_norm)
        )

        encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)
        encoding_indices = encoding_indices.view(*z.shape[:-1])

        z_q = self.embedding(encoding_indices).view(z.shape)
        z_qnorm, z_norm = self.norm(z_q), self.norm(z)

        # compute loss for embedding
        loss = self.beta * torch.mean((z_qnorm.detach() - z_norm) ** 2) + torch.mean(
            (z_qnorm - z_norm.detach()) ** 2
        )

        return z_qnorm, loss, encoding_indices

    def forward(
        self, z: torch.FloatTensor
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor]:
        if not self.use_residual:
            z_q, loss, encoding_indices = self.quantize(z)
        else:
            z_q = torch.zeros_like(z)
            residual = z.detach().clone()

            losses = []
            encoding_indices = []

            for _ in range(self.num_quantizers):
                z_qi, loss, indices = self.quantize(residual.clone())
                residual.sub_(z_qi)
                z_q.add_(z_qi)

                encoding_indices.append(indices)
                losses.append(loss)

            losses, encoding_indices = map(
                partial(torch.stack, dim=-1), (losses, encoding_indices)
            )
            loss = losses.mean()

        # preserve gradients with straight-through estimator
        if self.straight_through:
            z_q = z + (z_q - z).detach()

        return z_q, loss, encoding_indices