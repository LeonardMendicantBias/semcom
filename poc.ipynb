{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e03c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.reader import KineticDataset\n",
    "\n",
    "from src.vqgan import ViTVQGAN\n",
    "from src.my_model import MaskCode\n",
    "from src.trainer import AttentionMaskModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8ddb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/246534"
     ]
    }
   ],
   "source": [
    "path = \"E:/kinetics-dataset/k400\"\n",
    "split = \"train\"\n",
    "\n",
    "ds = KineticDataset(\n",
    "    path, split,\n",
    "    n_frames=32,\n",
    ")\n",
    "ds_loader = DataLoader(\n",
    "    ds, 1, True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds_loader:\n",
    "    code, _ = batch\n",
    "    break\n",
    "\n",
    "B, T, HW = code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd189a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = AttentionMaskModeling(\n",
    "    model=MaskCode(\n",
    "\t\twindow_size=(3, 3),\n",
    "\t\tlength=128, height=32, width=32,\n",
    "\t\tdepth=4, heads=12, dim=96, embed_dim=32,\n",
    "\t\tn_codes=8192,\n",
    "\t), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = AttentionMaskModeling(\n",
    "    model=MaskCode(\n",
    "\t\twindow_size=(3, 3),\n",
    "\t\tlength=32, height=32, width=32,\n",
    "\t\tdepth=4, heads=12, dim=96, embed_dim=32,\n",
    "\t\tn_codes=8192,\n",
    "\t), \n",
    ")\n",
    "\n",
    "lightning_model.cuda()\n",
    "_batch = batch[0].cuda(), None\n",
    "lightning_model.training_step(_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bcb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngoak\\anaconda3\\envs\\semcom\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:210: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "c:\\Users\\ngoak\\anaconda3\\envs\\semcom\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\ngoak\\anaconda3\\envs\\semcom\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:751: Checkpoint directory .\\semcom\\7shjo9he\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\ngoak\\anaconda3\\envs\\semcom\\lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | MaskCode           | 2.9 M  | train\n",
      "1 | teacher | MaskCode           | 2.9 M  | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "2.9 M     Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.285    Total estimated model params size (MB)\n",
      "213       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\ngoak\\anaconda3\\envs\\semcom\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cf4ebe4182452eb30792fe8d24011a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngoak\\anaconda3\\envs\\semcom\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "is_dev = False\n",
    "\n",
    "lightning_model = AttentionMaskModeling(\n",
    "    model=MaskCode(\n",
    "\t\twindow_size=(3, 3),\n",
    "\t\tlength=32, height=32, width=32,\n",
    "\t\tdepth=4, heads=8, dim=128, embed_dim=32,\n",
    "\t\tn_codes=8192,\n",
    "\t), \n",
    "    top_p=0.95\n",
    ")\n",
    "wandb_logger = WandbLogger(\n",
    "\tproject=\"semcom\",\n",
    ")\n",
    "# wandb_logger.experiment.config.update({\n",
    "#     \"dim\": dim,\n",
    "#     \"depth\": depth\n",
    "# })\n",
    "trainer = pl.Trainer(\n",
    "    # training settings\n",
    "    max_epochs=25,\n",
    "    val_check_interval=1.0,\n",
    "    accelerator=\"cpu\" if is_dev else \"gpu\",\n",
    "    precision=\"32-true\" if is_dev else \"16-mixed\",\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    # logging settings\n",
    "    default_root_dir=f\"./checkpoints\",\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "\t\t\tmonitor=\"val_loss\",\n",
    "\t\t\tdirpath=\"my/path/\",\n",
    "\t\t\tfilename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\",\n",
    "\t\t\tsave_top_k=3,\n",
    "\t\t\tmode=\"min\",\n",
    "\t\t)\n",
    "\t]\n",
    "    # dev setting\n",
    "    fast_dev_run=is_dev,\n",
    ")\n",
    "trainer.fit(\n",
    "    lightning_model, \n",
    "    train_dataloaders=ds_loader,\n",
    "    # val_dataloaders=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
