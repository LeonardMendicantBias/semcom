{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e03c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d9d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.reader import KineticDataset\n",
    "\n",
    "from src.vqgan import ViTVQGAN\n",
    "from src.my_model import MaskCode\n",
    "from src.trainer import AttentionMaskModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8ddb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/246534"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/e/kinetics-dataset/k400\"\n",
    "split = \"train\"\n",
    "\n",
    "ds = KineticDataset(\n",
    "    path, split,\n",
    "    n_frames=16,\n",
    ")\n",
    "ds_loader = DataLoader(\n",
    "    ds, 1, True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630bcb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | MaskCode           | 2.9 M  | train\n",
      "1 | teacher | MaskCode           | 2.9 M  | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "2.9 M     Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.309    Total estimated model params size (MB)\n",
      "213       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1337ef7eb5594362997be809aa75ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/anaconda3/envs/semcom/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "is_dev = False\n",
    "\n",
    "lightning_model = AttentionMaskModeling(\n",
    "    model=MaskCode(\n",
    "\t\twindow_size=(3, 3),\n",
    "\t\tlength=32, height=32, width=32,\n",
    "\t\tdepth=4, heads=8, dim=128, embed_dim=32,\n",
    "\t\tn_codes=8192,\n",
    "\t), \n",
    "    top_p=0.95\n",
    ")\n",
    "wandb_logger = WandbLogger(\n",
    "\tproject=\"semcom\",\n",
    ")\n",
    "# wandb_logger.experiment.config.update({\n",
    "#     \"dim\": dim,\n",
    "#     \"depth\": depth\n",
    "# })\n",
    "trainer = pl.Trainer(\n",
    "    # training settings\n",
    "    max_epochs=25,\n",
    "    val_check_interval=1.0,\n",
    "    accelerator=\"cpu\" if is_dev else \"gpu\",\n",
    "    precision=\"32-true\" if is_dev else \"16-mixed\",\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    # logging settings\n",
    "    default_root_dir=f\"./checkpoints\",\n",
    "    # logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "\t\t\tmonitor=\"train_acc\",\n",
    "\t\t\tdirpath=\"./trained\",\n",
    "\t\t\tfilename=\"semcom-{epoch:02d}-{train_acc:.2f}\",\n",
    "\t\t\tsave_top_k=3,\n",
    "\t\t\tmode=\"max\",\n",
    "\t\t)\n",
    "\t],\n",
    "    # dev setting\n",
    "    fast_dev_run=is_dev,\n",
    ")\n",
    "trainer.fit(\n",
    "    lightning_model, \n",
    "    train_dataloaders=ds_loader,\n",
    "    # val_dataloaders=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
