{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4d7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.reader import KineticDataset, KineticDatasetVideo\n",
    "\n",
    "from src.vqgan import ViTVQGAN\n",
    "from src.my_model import MaskCode, Encoder, Decoder, MaskVideo\n",
    "from src.trainer import AttentionMaskModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bd7e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/e/kinetics-dataset/k400\"\n",
    "split = \"train\"\n",
    "\n",
    "# ds = KineticDatasetVideo(\n",
    "#     path, split,\n",
    "#     n_frames=16,\n",
    "# )\n",
    "ds = KineticDatasetVideo.get_ds(path, split, 16)\n",
    "ds_loader = DataLoader(\n",
    "    ds, 2, True, \n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef1bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in ds_loader:\n",
    "    code, _ = batch\n",
    "    break\n",
    "\n",
    "_batch = batch[0].cuda(), None\n",
    "B, T, _, H, W = code.shape\n",
    "code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960bc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_finetune = False\n",
    "image_size = (256, 256)\n",
    "patch_size = (8, 8)\n",
    "depth, heads, dim, embed_dim = 12, 12, 768, 32\n",
    "\n",
    "window_size = (3, 3)\n",
    "length, height, width = 32, 32, 32\n",
    "temporal_depth, temporal_heads, temporal_dim = 4, 8, 128\n",
    "n_codes=8192\n",
    "\n",
    "vitvq_path = \"./checkpoint/imagenet_vitvq_base.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = AttentionMaskModeling(\n",
    "    model=MaskVideo(\n",
    "        is_finetune=is_finetune,\n",
    "        image_size=image_size, patch_size=patch_size,\n",
    "        depth=depth, heads=heads, dim=dim,\n",
    "        n_codes=n_codes, embed_dim=embed_dim,\n",
    "\n",
    "        window_size=window_size,\n",
    "        length=length, height=height, width=width,\n",
    "        temporal_depth=temporal_depth, temporal_heads=temporal_heads, temporal_dim=temporal_dim,\n",
    "\n",
    "        drop_prob=0.1, depth_prob=0.1,\n",
    "        vitvq_path=vitvq_path\n",
    "\t), \n",
    "    top_p=0.95\n",
    ")\n",
    "lightning_model.cuda()\n",
    "lightning_model.training_step(_batch, 0)\n",
    "\n",
    "# frames, _ = _batch  # (B, T, HW)\n",
    "# mask = lightning_model.get_mask_from_logits(frames)  # (B, T', HW)\n",
    "\n",
    "# _, _, attn_logits = lightning_model.teacher.encoder(frames, None, False)\n",
    "# attn_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    is_finetune=is_finetune,\n",
    "    image_size=image_size, patch_size=patch_size,\n",
    "    depth=depth, heads=heads, dim=dim,\n",
    "    window_size=window_size,\n",
    "    length=length, height=height, width=width,\n",
    "    temporal_depth=temporal_depth, temporal_heads=temporal_heads, temporal_dim=temporal_dim,\n",
    "    n_codes=n_codes, embed_dim=embed_dim,\n",
    "    drop_prob=0.1, depth_prob=0.1,\n",
    "    vitvq_path=vitvq_path\n",
    ")\n",
    "encoder.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ef1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    code, logits, attn = encoder(_batch[0])\n",
    "code.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    is_finetune=is_finetune,\n",
    "    image_size=image_size, patch_size=patch_size,\n",
    "    depth=depth, heads=heads, dim=dim,\n",
    "    n_codes=n_codes, embed_dim=embed_dim,\n",
    "    drop_prob=0.1, depth_prob=0.1,\n",
    "    vitvq_path=vitvq_path\n",
    ")\n",
    "decoder.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752e2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    output = decoder(code)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ad1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/leonard/anaconda3/envs/semcom/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/leonard/anaconda3/envs/semcom/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /home/leonard/semcom/trained exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/leonard/anaconda3/envs/semcom/lib/python3.10/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type               | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model   | MaskVideo          | 175 M  | train\n",
      "1 | teacher | MaskVideo          | 175 M  | train\n",
      "2 | acc     | MulticlassAccuracy | 0      | train\n",
      "-------------------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "347 M     Non-trainable params\n",
      "350 M     Total params\n",
      "1,403.305 Total estimated model params size (MB)\n",
      "1069      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b447e6f83641bbbf76dbdf52001911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/anaconda3/envs/semcom/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    }
   ],
   "source": [
    "is_dev = False\n",
    "\n",
    "lightning_model = AttentionMaskModeling(\n",
    "    model=MaskVideo(\n",
    "        is_finetune=is_finetune,\n",
    "        image_size=image_size, patch_size=patch_size,\n",
    "        depth=depth, heads=heads, dim=dim,\n",
    "        n_codes=n_codes, embed_dim=embed_dim,\n",
    "\n",
    "        window_size=window_size,\n",
    "        length=length, height=height, width=width,\n",
    "        temporal_depth=temporal_depth, temporal_heads=temporal_heads, temporal_dim=temporal_dim,\n",
    "\n",
    "        drop_prob=0.1, depth_prob=0.1,\n",
    "        vitvq_path=vitvq_path\n",
    "\t), \n",
    "    top_p=0.95\n",
    ")\n",
    "# wandb_logger = WandbLogger(\n",
    "# \tproject=\"semcom\",\n",
    "# )\n",
    "# wandb_logger.experiment.config.update({\n",
    "#     \"dim\": dim,\n",
    "#     \"depth\": depth\n",
    "# })\n",
    "trainer = pl.Trainer(\n",
    "    # training settings\n",
    "    max_epochs=25,\n",
    "    val_check_interval=1.0,\n",
    "    accelerator=\"cpu\" if is_dev else \"gpu\",\n",
    "    precision=\"32-true\" if is_dev else \"16-mixed\",\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val=1.0,\n",
    "    # logging settings\n",
    "    default_root_dir=f\"./checkpoints\",\n",
    "    # logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "\t\t\tmonitor=\"train_acc\",\n",
    "\t\t\tdirpath=\"./trained\",\n",
    "\t\t\tfilename=\"semcom-{epoch:02d}-{train_acc:.2f}\",\n",
    "\t\t\tsave_top_k=3,\n",
    "\t\t\tmode=\"max\",\n",
    "\t\t)\n",
    "\t],\n",
    "    # dev setting\n",
    "    fast_dev_run=is_dev,\n",
    ")\n",
    "trainer.fit(\n",
    "    lightning_model, \n",
    "    train_dataloaders=ds_loader,\n",
    "    # val_dataloaders=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
